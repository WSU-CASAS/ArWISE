# generate_features.py --inputfile <input_file> --outputfile <output_file>
#   [--backfill 0] [--downsample None] [--timewindow] [--raw]
#   [--windowsize 300] [--stepsize 300] [--mindata 1]
#
# Generate features from given input file in CSV format. The input file may
# not have columns 'activity_label', 'is_gps_valid', 'notes'; in which case,
# those columns are added with None values. Generated features are written
# to the given output file.
#
# The --backfill option specifies the number of seconds into the past to copy
# a user_activity_label. Backfill is performed first, before the other
# processing. The default of 0 means no backfill.
#
# If a --downsample rate is given (in samples/second Hz), then the data is
# resampled at this rate. The downsample rate is assumed to be less than the
# original sampling rate of the data. The default is no downsampling. This
# is down after backfill, but before feature computation.
#
# By default, features are generated as aggregated values over the window.
# But if the --raw option is given, then all the values over all the rows
# in the window are used as features, not the aggregated values. See the
# 'raw_features' list in 'compute_features_raw' for the raw features that
# are extracted.
#
# By default, each output row has a 'stamp' timestamp feature, which is the
# latest time in the window of data used to generate the feature values. If the
# --timewindow option is given, then both the start and end times are included
# in the features using feature names 'stamp_start' and 'stamp_end'.
#
# Features are generated by passing a window of size --windowsize (in seconds,
# default=300) over the data, advancing the window by --stepsize (in seconds,
# default=300). Each window must have at least --mindata (default 1) entries
# from the downsampled input data file to be a viable window.
#
# You can add new features in the 'compute_features' method.

import pandas as pd
import numpy as np
import argparse
import math
from geopy.distance import geodesic

GPS_DIGITS_PRECISION = 4 # 4 decimal places is 10m precision
DEFAULT_HOME_LOCATION = (46.7335, -117.1996) # Pullman, WA (for the rare case of no location data)
g_home_location = DEFAULT_HOME_LOCATION # Global variable that should be set before feature generation

# Define the column types
COLUMN_TYPES = {
    'yaw': 'float64',
    'pitch': 'float64',
    'roll': 'float64',
    'rotation_rate_x': 'float64',
    'rotation_rate_y': 'float64',
    'rotation_rate_z': 'float64',
    'user_acceleration_x': 'float64',
    'user_acceleration_y': 'float64',
    'user_acceleration_z': 'float64',
    'latitude': 'float64',
    'longitude': 'float64',
    'altitude': 'float64',
    'course': 'float64',
    'speed': 'float64',
    'horizontal_accuracy': 'float64',
    'vertical_accuracy': 'float64',
    'battery_state': 'object',  # possible values: nan, unplugged, charging, full
    'user_activity_label': 'object',
    'activity_label': 'object', # may not be in data file; if not, add as None
    'is_gps_valid': 'object',   # may not be in data file; if not, add as None
    'notes': 'object'           # may not be in data file; if not, add as None
}

# Define columns to parse as dates (if any)
# First column in list will be used as index and for windowing calculations.
PARSE_DATES = ['stamp']

# Define methods for aggregating data for downsampling
DOWNSAMPLE_METHODS = {
    'yaw': 'mean',
    'pitch': 'mean',
    'roll': 'mean',
    'rotation_rate_x': 'mean',
    'rotation_rate_y': 'mean',
    'rotation_rate_z': 'mean',
    'user_acceleration_x': 'mean',
    'user_acceleration_y': 'mean',
    'user_acceleration_z': 'mean',
    'latitude': 'mean',
    'longitude': 'mean',
    'altitude': 'mean',
    'course': 'mean',
    'speed': 'mean',
    'horizontal_accuracy': 'mean',
    'vertical_accuracy': 'mean',
    'battery_state': lambda x: most_frequent(x),
    'user_activity_label': lambda x: most_frequent(x),
    'activity_label': lambda x: most_frequent(x),
    'is_gps_valid': lambda x: most_frequent(x),
    'notes': lambda x: most_frequent(x)
}

def compute_features(df):
    """Compute features over given window of data. Return features as a dictionary {feature_name: value}.
    Be sure to call set_home_location before this function.
    Can add new features here, or comment out those not needed."""
    global g_home_location
    features_dict = dict()
    # Make a copy of df window slice so can add columns without affecting original df.
    df = df.copy()

    # Set timestamps to actual start and end of window
    features_dict['stamp_start'] = df.index.min()
    features_dict['stamp_end'] = df.index.max()

    # Compute mean and stddev of yaw, pitch, row, acceleration x/y/z, rotation x/y/z
    features_dict['yaw_mean'] = df['yaw'].mean()
    features_dict['pitch_mean'] = df['pitch'].mean()
    features_dict['roll_mean'] = df['roll'].mean()
    features_dict['rotation_rate_x_mean'] = df['rotation_rate_x'].mean()
    features_dict['rotation_rate_y_mean'] = df['rotation_rate_y'].mean()
    features_dict['rotation_rate_z_mean'] = df['rotation_rate_z'].mean()
    features_dict['user_acceleration_x_mean'] = df['user_acceleration_x'].mean()
    features_dict['user_acceleration_y_mean'] = df['user_acceleration_x'].mean()
    features_dict['user_acceleration_z_mean'] = df['user_acceleration_z'].mean()
    features_dict['yaw_std'] = df['yaw'].std()
    features_dict['pitch_std'] = df['pitch'].std()
    features_dict['roll_std'] = df['roll'].std()
    features_dict['rotation_rate_x_std'] = df['rotation_rate_x'].std()
    features_dict['rotation_rate_y_std'] = df['rotation_rate_y'].std()
    features_dict['rotation_rate_z_std'] = df['rotation_rate_z'].std()
    features_dict['user_acceleration_x_std'] = df['user_acceleration_x'].std()
    features_dict['user_acceleration_y_std'] = df['user_acceleration_x'].std()
    features_dict['user_acceleration_z_std'] = df['user_acceleration_z'].std()

    # Compute mean and stddev of magnitude of rotation and acceleration
    # First compute auxiliary magnitudes for each row in window
    df['rotation_magnitude'] = np.sqrt(df['rotation_rate_x']**2 + df['rotation_rate_y']**2 + df['rotation_rate_z']**2)
    df['acceleration_magnitude'] = np.sqrt(df['user_acceleration_x']**2 + df['user_acceleration_y']**2 + df['user_acceleration_z']**2)
    features_dict['rotation_magnitude_mean'] = df['rotation_magnitude'].mean()
    features_dict['acceleration_magnitude_mean'] = df['acceleration_magnitude'].mean()
    features_dict['rotation_magnitude_std'] = df['rotation_magnitude'].std()
    features_dict['acceleration_magnitude_std'] = df['acceleration_magnitude'].std()
    
    # Compute speed mean and stddev, most frequent course, and stddev course
    features_dict['speed_mean'] = df['speed'].mean()
    features_dict['speed_std'] = df['speed'].std()
    features_dict['course_mode'] = most_frequent(df['course'].round())
    features_dict['course_std'] = df['course'].std()

    # Compute mean distance from home and bearing from home
    # First compute auxiliary distance and bearing for each row in window
    df['distance_from_home'] = df.apply(distance_from_home, axis=1)
    df['bearing_from_home'] = df.apply(bearing_from_home, axis=1)
    df['distance_from_home_latitude'] = abs(df['latitude'] - g_home_location[0])
    df['distance_from_home_longitude'] = abs(df['longitude'] - g_home_location[1])
    features_dict['distance_from_home_mean'] = df['distance_from_home'].mean()
    features_dict['distance_from_home_std'] = df['distance_from_home'].std()
    features_dict['distance_from_home_latitude_mean'] = df['distance_from_home_latitude'].mean()
    features_dict['distance_from_home_latitude_std'] = df['distance_from_home_latitude'].std()
    features_dict['distance_from_home_longitude_mean'] = df['distance_from_home_longitude'].mean()
    features_dict['distance_from_home_longitude_std'] = df['distance_from_home_longitude'].std()
    features_dict['bearing_from_home_mode'] = most_frequent(df['bearing_from_home'].round())
    features_dict['bearing_from_home_std'] = df['bearing_from_home'].std()

    # Compute time of day in radians using minutes from midnight of last timestamp in window
    # I.e., 2 * pi * (minutes to window / minutes_per_day)
    # Also compute sine and cosine of this value.
    minutes_in_day = 24 * 60
    timestamp = features_dict['stamp_end']
    minutes_to_window = (timestamp.hour * 60) + timestamp.minute
    features_dict['time_of_day_radians'] = 2 * np.pi * minutes_to_window / minutes_in_day
    features_dict['time_of_day_sin'] = np.sin(features_dict['time_of_day_radians'])
    features_dict['time_of_day_cos'] = np.cos(features_dict['time_of_day_radians'])
    features_dict['day_of_week'] = float(timestamp.weekday()) # (0 = Monday, 6 = Sunday)

    # This should be the last feature, which is the activity label.
    # First check for non-Null user_activity_label at the end of the window.
    # Otherwise, check for non-Null activity_label at the end of the window.
    # Otherwise, set to None.
    activity_label = None
    act_last = df.iloc[-1]['user_activity_label']
    if not pd.isna(act_last):
        activity_label = act_last
    else:
        act_last = df.iloc[-1]['activity_label']
        if not pd.isna(act_last):
            activity_label = act_last
    features_dict['activity_label'] = activity_label

    return features_dict

def compute_features_raw(df):
    """Compute features over given window of data (df). Return features as a dictionary {feature_name: value}.
    Be sure to call set_home_location before this function. This version collects all raw data features over
    all rows in window (df): acceleration, rotation, distance/bearing from home."""
    global g_home_location
    features_dict = dict()
    # Make a copy of df window slice so can add columns without affecting original df.
    df = df.copy()

    # Compute distance and bearing for each row in window
    df['distance_from_home'] = df.apply(distance_from_home, axis=1)
    df['bearing_from_home'] = df.apply(bearing_from_home, axis=1)

    # Set timestamps to actual start and end of window
    features_dict['stamp_start'] = df.index.min()
    features_dict['stamp_end'] = df.index.max()

    raw_features = ['user_acceleration_x', 'user_acceleration_y', 'user_acceleration_z',
                    'rotation_rate_x', 'rotation_rate_y', 'rotation_rate_z',
                    'distance_from_home', 'bearing_from_home']
    raw_features_dict = {f"{col}_{i+1}": df.iloc[i][col] for i in range(len(df)) for col in raw_features}
    features_dict.update(raw_features_dict)

    # This should be the last feature, which is the activity label.
    # First check for non-Null user_activity_label at the end of the window.
    # Otherwise, check for non-Null activity_label at the end of the window.
    # Otherwise, set to None.
    activity_label = None
    act_last = df.iloc[-1]['user_activity_label']
    if not pd.isna(act_last):
        activity_label = act_last
    else:
        act_last = df.iloc[-1]['activity_label']
        if not pd.isna(act_last):
            activity_label = act_last
    features_dict['activity_label'] = activity_label

    return features_dict

def magnitude_mean(data, column1, column2, column3):
    return np.mean(np.sqrt(data[column1]**2 + data[column2]**2 + data[column3]**2))

def magnitude_std(data, column1, column2, column3):
    return np.std(np.sqrt(data[column1]**2 + data[column2]**2 + data[column3]**2))

def set_home_location(df):
    """Set home location to most frequent lat/lon between midnight and 8am. If no valid GPS values
    in that range, then try whole dataframe. If still no valid GPS values, then use default. """
    global g_home_location
    g_home_location = DEFAULT_HOME_LOCATION
    filtered_df = df[df['latitude'].notna() & df['longitude'].notna()]
    if len(filtered_df) > 0:
        locations_df = filtered_df.round({'latitude': GPS_DIGITS_PRECISION, 'longitude': GPS_DIGITS_PRECISION})
        morning_locations_df = locations_df[(locations_df.index.hour >=0) & (locations_df.index.hour < 8)]
        if len(morning_locations_df) > 0:
            g_home_location = (morning_locations_df.groupby(['latitude', 'longitude']).size().idxmax())
        else:
            g_home_location = (locations_df.groupby(['latitude', 'longitude']).size().idxmax())
    return

def distance_from_home(row):
    """Compute the mean distance from home in meters.
    Be sure to call set_home_location before calling this function."""
    global g_home_location
    if pd.isna(row['latitude']) or pd.isna(row['longitude']) or row['is_gps_valid'] == '0':
        return np.nan
    else:
        return geodesic(g_home_location, (row['latitude'], row['longitude'])).meters

def bearing_from_home(row):
    """Compute the mean bearing from home in degrees.
    Be sure to call set_home_location before calling this function."""
    global g_home_location
    if pd.isna(row['latitude']) or pd.isna(row['longitude']) or row['is_gps_valid'] == '0':
        return np.nan
    else:
        return bearing(g_home_location[0], g_home_location[1], row['latitude'], row['longitude'])

def bearing(lat1, lon1, lat2, lon2):
    """Compute bearing from lat1/lon1 to lat2/lon2 in degrees."""
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    x = math.sin(dlon) * math.cos(lat2)
    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)
    bearing = math.atan2(x, y)
    bearing = math.degrees(bearing)
    # Normalize the bearing to a compass direction (0 to 360 degrees)
    bearing = (bearing + 360) % 360
    return bearing

def most_frequent(data):
    return data.mode().iloc[0] if not data.mode().empty else None

def compute_features_with_window(df, window_size, step_size, min_data, raw_features):
    """Moves a time-based window over the data and computes features from the data
    in the window. The window size and the amount of shift (step size) are given.
    Each window must have at least min_data entries to be viable. If raw_features is
    True, then all the raw features are added; otherwise, the aggregated features
    over the window are used. Returns a dataframe containing the generated features.
    """
    set_home_location(df)
    features = []
    start_time = df.index.min()
    end_time = df.index.max()

    current_start = start_time
    while current_start < end_time:
        current_end = current_start + pd.Timedelta(window_size)
        window_data = df[current_start:current_end]
        if not window_data.empty:
            if len(window_data) >= min_data:
                if raw_features:
                    feature_dict = compute_features_raw(window_data)
                else:
                    feature_dict = compute_features(window_data)
                features.append(feature_dict)
            current_start += pd.Timedelta(step_size)
        else:
            # Advance window to next available timestamp
            later_idx = df.index.searchsorted(current_start)
            current_start = df.index[later_idx]
    
    return pd.DataFrame(features)

def downsample(df, rate, sampling_methods):
    """
    Each sample has the timestamp of the start of the sample, and the remaining
    columns are aggregated according to the given sampling methods.
    """
    downsample_df = df.resample(rate).agg(sampling_methods)
    print(f'Downsampled from {len(df)} to {len(downsample_df)} rows')
    return downsample_df

def backfill(df, backfill_window):
    """Copy a non-null user_activity_label value to all previous rows within backfill_window seconds."""
    time_window = pd.Timedelta(seconds=backfill_window)
    # Iterate through the DataFrame in order
    for i in range(len(df)):
        current_activity = df.iloc[i]["user_activity_label"]
        if pd.notna(current_activity):
            window_start = df.index[i] - time_window
            # Look backward within the time window
            for j in range(i - 1, -1, -1):  # Iterate backward
                if df.index[j] < window_start:  # Stop if out of time range
                    break
                if pd.isna(df.iloc[j]["user_activity_label"]):  # Fill only NaN values
                    df.iloc[j, df.columns.get_loc("user_activity_label")] = current_activity
    return df

def read_csv_with_types(file_path, column_types, parse_dates=None):
    """
    Reads a CSV file into a Pandas DataFrame with specified data types for each column.

    Parameters:
        file_path (str): Path to the CSV file.
        column_types (dict): A dictionary mapping column names to their data types.
        parse_dates (list, optional): List of column names to parse as dates.

    Returns:
        pd.DataFrame: The resulting DataFrame with specified column types.
    """
    # Read the CSV file (skipping 2nd row of type symbols)
    df = pd.read_csv(file_path, skiprows=[1], low_memory=False)

    # Check for missing columns and add them to the DataFrame with None values
    missing_columns = [col for col in column_types if col not in df.columns]
    if missing_columns:
        #print(f"Warning: The following columns are missing from the CSV file and will be added with None values: {missing_columns}")
        for col in missing_columns:
            df[col] = None

    # Apply the specified data types to the DataFrame
    for column, dtype in column_types.items():
        df[column] = df[column].astype(dtype)
    
    # Remove rows with battery_state = charging or full, i.e., watch is on the charger
    df = df[~df['battery_state'].isin(['charging', 'full'])]

    # Set the timestamp column as the index and sort the DataFrame by it
    if parse_dates:
        # Change date columns to datetime type
        for dt_column in parse_dates:
            df[dt_column] = pd.to_datetime(df[dt_column], format='mixed')
        index_dt_column = parse_dates[0]
        df.set_index(index_dt_column, inplace=True)
        df.sort_index(inplace=True) # just in case
    else:
        print('No timestamp column provided. Add to parse_dates.')

    return df

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--inputfile', dest='input_file', type=str, required=True)
    parser.add_argument('--outputfile', dest='output_file', type=str, required=True)
    parser.add_argument('--windowsize', dest='window_size', type=int, default=300)
    parser.add_argument('--stepsize', dest='step_size', type=int, default=300)
    parser.add_argument('--mindata', dest='min_data', type=int, default=1)
    parser.add_argument('--downsample', dest='downsample_rate', type=int, default=None)
    parser.add_argument('--timewindow', dest='time_window', action='store_true')
    parser.add_argument('--raw', dest='raw_features', action='store_true')
    parser.add_argument('--backfill', dest='backfill', type=int, default=0)
    args = parser.parse_args()
    return args  

if __name__ == "__main__":
    args = parse_arguments() 
    
    # Read the CSV into a DataFrame
    df = read_csv_with_types(args.input_file, COLUMN_TYPES, PARSE_DATES)

    # Backfill user's activity label
    if args.backfill > 0:
        df = backfill(df, args.backfill)

    # Downsample
    if args.downsample_rate:
        downsample_rate = str(args.downsample_rate) + 's'
        df = downsample(df, downsample_rate, DOWNSAMPLE_METHODS)

    # Build new dataframe of features
    window_size = str(args.window_size) + 's'
    step_size = str(args.step_size) + 's'
    min_data = args.min_data
    raw_features = args.raw_features

    features_df = compute_features_with_window(df, window_size, step_size, min_data, raw_features)

    if not args.time_window:
        # Drop 'stamp_start' column and rename 'stamp_end' column to 'stamp'
        features_df = features_df.drop(columns=['stamp_start'])
        features_df = features_df.rename(columns={'stamp_end': 'stamp'})

    # Write new dataframe to output CSV file
    features_df.to_csv(args.output_file, index=False)
